{"cells":[{"cell_type":"markdown","metadata":{"id":"OkQkyLfWx8DZ"},"source":["# Projeto 1 - módulo 6"]},{"cell_type":"markdown","metadata":{"id":"Dz7ar2ek1kmV"},"source":["## Precificação dinâmica - e-commerce"]},{"cell_type":"markdown","metadata":{"id":"A5KDmRb_3OFW"},"source":["### Mercari Price Suggestion Challenge - Kaggle\n","\n","Mercari é um site de revenda de produtos online. Uma dos desafios desse tipo de plataforma é auxiliar o usuário, muitas vezes com pouco conhecimento de vendas, a determinar um preço para os seus produtos de modo a maximizar as chances de venda.\n","\n","### Sobre este projeto\n","\n","O presente projeto tem o objetivo de desenvolver um algoritmo que identifique produtos já vendidos similares e sugira ao usuário um preço ótimo para novos produtos cadastrados.\n"]},{"cell_type":"markdown","metadata":{"id":"W74OS4x9B-_Q"},"source":["### Preparação do ambiente\n","\n","Para este projeto, acesse o link https://www.kaggle.com/competitions/mercari-price-suggestion-challenge/overview \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3806,"status":"ok","timestamp":1665456952091,"user":{"displayName":"lucio fontes","userId":"06607750709923064447"},"user_tz":180},"id":"nPDvIOK2x6-l","outputId":"c0d35da5-26d7-4710-dfaf-c01e2b878195"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import gensim.downloader as api\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","\n","!pip install tensorflow_addons\n","import tensorflow_addons as tfa\n","from sklearn import metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3072,"status":"ok","timestamp":1665456955150,"user":{"displayName":"lucio fontes","userId":"06607750709923064447"},"user_tz":180},"id":"NK3DylutMeVe","outputId":"beb3d1b9-0842-4967-ee9b-3c36e099d87e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qu95X3LmCQ4E"},"outputs":[],"source":["base_train = pd.read_csv('/content/drive/MyDrive/Blue Edtech/data/processed/dados_treino.csv') \n","base_test = pd.read_csv('/content/drive/MyDrive/Blue Edtech/data/processed/dados_teste.csv')\n","base_val = pd.read_csv('/content/drive/MyDrive/Blue Edtech/data/processed/dados_validacao.csv') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1v93MXxR-4rN"},"outputs":[],"source":["# preenchendo dados NAN\n","\n","base_train['item_description'] = base_train['item_description'].fillna(\"No description\")\n","base_train['name'] = base_train['name'].fillna(\" \")\n","\n","base_test['item_description'] = base_test['item_description'].fillna(\"No description\")\n","base_test['name'] = base_test['name'].fillna(\" \")\n","\n","base_val['item_description'] = base_val['item_description'].fillna(\"No description\")\n","base_val['name'] = base_val['name'].fillna(\" \")"]},{"cell_type":"code","source":["# unindo os atributos textuais buscando fortalecer a relação semântica entre eles\n","\n","base_train['name_brand_description'] = base_train['name'] + ' ' + base_train['brand_name'] + ' ' + base_train['item_description']\n","base_test['name_brand_description'] = base_test['name'] + ' ' + base_test['brand_name'] + ' ' + base_test['item_description']\n","base_val['name_brand_description'] = base_val['name'] + ' ' + base_val['brand_name'] + ' ' + base_val['item_description']"],"metadata":{"id":"UQhIsQBXKgxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPSpy_e0oUs3"},"outputs":[],"source":["# função de tokenização e preenchimento de comprimento\n","\n","def text_vectorizer(feature):\n","\n","  # TOKENIZAÇÃO\n","  tk = Tokenizer()\n","\n","  # FIT ON TRAIN \n","  tk.fit_on_texts(base_train[feature].apply(str))\n","\n","  # TOKENIZANDO O DATASET DE TREINO\n","  tk_train = tk.texts_to_sequences(base_train[feature].apply(str))\n","\n","  # TOKENIZANDO O DATASET DE TESTE\n","  tk_test = tk.texts_to_sequences(base_test[feature].apply(str))\n","\n","  # TOKENIZANDO O DATASET DE VALIDAÇÃO\n","  tk_val = tk.texts_to_sequences(base_val[feature].apply(str))\n","\n","  # COMPUTANDO O COMPRIMENTO MÁXIMO\n","  max_length = base_train[feature].apply(lambda x :len(str(x).split())).max()\n","\n","  # COMPUTANDO O TAMANHO DO VOCABULÁRIO\n","  vocab_size = len(tk.word_index) + 1\n","\n","  # PADDING A SEQUENCIA DE TREINO\n","  train_pad= pad_sequences(tk_train,padding=\"post\",maxlen = max_length)\n","\n","  # PADDING A SEQUENCIA DE TESTE\n","  test_pad = pad_sequences(tk_test,padding = \"post\", maxlen = max_length)\n","\n","  # PADDING A SEQUENCIA DE TESTE\n","  val_pad = pad_sequences(tk_val,padding = \"post\", maxlen = max_length)\n","\n","  # RETURNANDO O COMPRIMENTO MÁXIMO, SEQUÊNCIA PADDED DE TRAINO  , SEQUENCIA PADDED DE VALIDAÇÃO \n","  return max_length, vocab_size, train_pad , test_pad, val_pad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsIgVcxLqwfc"},"outputs":[],"source":["# rodando a função text_vectorizer para todos os atributos\n","\n","max_length_name_brand_description, vocab_size_name_brand_description, train_name_brand_description_pad , test_name_brand_description_pad, val_name_brand_description_pad = text_vectorizer('name_brand_description')\n","max_length_category_1, vocab_size_category_1, train_category_1_pad , test_category_1_pad, val_category_1_pad = text_vectorizer('category_1')\n","max_length_category_2, vocab_size_category_2, train_category_2_pad , test_category_2_pad, val_category_2_pad = text_vectorizer('category_2')\n","max_length_category_3, vocab_size_category_3, train_category_3_pad , test_category_3_pad, val_category_3_pad = text_vectorizer('category_3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKkwXHUCQ_UD"},"outputs":[],"source":["# separando treino e teste do atributo shipping\n","\n","train_shipping = base_train.shipping\n","test_shipping = base_test.shipping\n","val_shipping = base_val.shipping\n","\n","# aplicando one hot encoding para transformar dados categóricos em uma representação vetorial binária e\n","# separando treino e teste do atributo item_condition_id\n","\n","base_train_item_condition = pd.get_dummies(base_train['item_condition_id'])\n","base_test_item_condition = pd.get_dummies(base_test['item_condition_id'])\n","base_val_item_condition = pd.get_dummies(base_val['item_condition_id'])\n","train_item_cond = base_train_item_condition.to_numpy()\n","test_item_cond = base_test_item_condition.to_numpy()\n","val_item_cond = base_val_item_condition.to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5UUoUt7O0ZR"},"outputs":[],"source":["# armazenando os dados em uma lista\n","\n","x_train = [train_item_cond,train_shipping,train_category_1_pad,train_category_2_pad,train_category_3_pad,train_name_brand_description_pad]\n","x_test = [test_item_cond,test_shipping,test_category_1_pad, test_category_2_pad, test_category_3_pad,test_name_brand_description_pad]\n","x_val = [val_item_cond,val_shipping,val_category_1_pad, val_category_2_pad, val_category_3_pad,val_name_brand_description_pad]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGruUoWTRWLw"},"outputs":[],"source":["# convertendo formato do atributo price para log\n","\n","base_train['log_price'] = np.log(base_train['price'])\n","base_test['log_price'] = np.log(base_test['price'])\n","\n","y_train = base_train.log_price\n","y_test = base_test.log_price"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k59jrxvRoy7v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665457168386,"user_tz":180,"elapsed":616,"user":{"displayName":"lucio fontes","userId":"06607750709923064447"}},"outputId":"44754f60-b23f-4628-d504-81afe5e0aa4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_6 (InputLayer)           [(None, 254)]        0           []                               \n","                                                                                                  \n"," input_1 (InputLayer)           [(None, 5)]          0           []                               \n","                                                                                                  \n"," input_3 (InputLayer)           [(None, 3)]          0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 5)]          0           []                               \n","                                                                                                  \n"," input_5 (InputLayer)           [(None, 7)]          0           []                               \n","                                                                                                  \n"," embedding_4 (Embedding)        (None, 254, 20)      3853940     ['input_6[0][0]']                \n","                                                                                                  \n"," embedding (Embedding)          (None, 5, 10)        60          ['input_1[0][0]']                \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 3, 16)        240         ['input_3[0][0]']                \n","                                                                                                  \n"," embedding_2 (Embedding)        (None, 5, 16)        2336        ['input_4[0][0]']                \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, 7, 40)        38240       ['input_5[0][0]']                \n","                                                                                                  \n"," gru (GRU)                      (None, 254, 64)      16512       ['embedding_4[0][0]']            \n","                                                                                                  \n"," flatten (Flatten)              (None, 50)           0           ['embedding[0][0]']              \n","                                                                                                  \n"," dense (Dense)                  (None, 10)           20          ['input_2[0][0]']                \n","                                                                                                  \n"," flatten_1 (Flatten)            (None, 48)           0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," flatten_2 (Flatten)            (None, 80)           0           ['embedding_2[0][0]']            \n","                                                                                                  \n"," flatten_3 (Flatten)            (None, 280)          0           ['embedding_3[0][0]']            \n","                                                                                                  \n"," flatten_4 (Flatten)            (None, 16256)        0           ['gru[0][0]']                    \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 16724)        0           ['flatten[0][0]',                \n","                                                                  'dense[0][0]',                  \n","                                                                  'flatten_1[0][0]',              \n","                                                                  'flatten_2[0][0]',              \n","                                                                  'flatten_3[0][0]',              \n","                                                                  'flatten_4[0][0]']              \n","                                                                                                  \n"," dense_1 (Dense)                (None, 512)          8563200     ['concatenate[0][0]']            \n","                                                                                                  \n"," dropout (Dropout)              (None, 512)          0           ['dense_1[0][0]']                \n","                                                                                                  \n"," dense_2 (Dense)                (None, 256)          131328      ['dropout[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 256)          0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 128)          32896       ['dropout_1[0][0]']              \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 128)         512         ['dropout_2[0][0]']              \n"," alization)                                                                                       \n","                                                                                                  \n"," dense_4 (Dense)                (None, 1)            129         ['batch_normalization[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 12,639,413\n","Trainable params: 12,639,157\n","Non-trainable params: 256\n","__________________________________________________________________________________________________\n"]}],"source":["# arquitetura do deep learning\n","\n","tf.keras.backend.clear_session()\n","# ITEM CONDITION ID\n","inp1 = layers.Input(shape=(5)) # INPUT 1 \n","emb1  = layers.Embedding(6,10,input_length= 5)(inp1) # EMBEDDING 1\n","flat1 = layers.Flatten()(emb1) # FLATTEN\n","# SHIPPING \n","inp2 = layers.Input(shape=(1)) # INPUT 2 \n","d2 = layers.Dense(10,activation=\"relu\")(inp2) # DENSE LAYER 2\n","# CATEGORY_1\n","inp3 = layers.Input(shape = (3)) # INPUT 4\n","emb3 = layers.Embedding(vocab_size_category_1, 16, input_length= 3)(inp3) # EMBEDDING 4\n","flat3 = layers.Flatten()(emb3) # FLATTEN \n","# CATEGORY_2\n","inp4 = layers.Input(shape = (5)) # INPUT 5\n","emb4 = layers.Embedding(vocab_size_category_2 , 16, input_length= 5)(inp4) # EMBEDDING 5\n","flat4 = layers.Flatten()(emb4) # FLATTEN\n","# CATEGORY_3\n","inp5= layers.Input(shape = (7)) # INPUT 6 \n","emb5 = layers.Embedding(vocab_size_category_3, 40, input_length= 7)(inp5) # EMBEDDING 6\n","flat5 = layers.Flatten()(emb5) # FLATTEN\n","# ITEM NAME_BRAND_DESCRIPTION\n","inp6= layers.Input(shape = (254)) # INPUT 8\n","emb6 = layers.Embedding(vocab_size_name_brand_description , 20 , input_length= 254 )(inp6) # EMBEDDING 8\n","lstm6 = layers.GRU(64,return_sequences=True)(emb6) # GRU\n","flat6 = layers.Flatten()(lstm6) # FLATTEN\n","# CONCATENAÇÃO\n","concat = layers.Concatenate()([flat1,d2,flat3,flat4,flat5,flat6])\n","# DENSE LAYERS\n","dense1 = layers.Dense(512,activation=\"relu\")(concat)\n","# DROPOUT LAYER\n","drop2 = layers.Dropout(0.2)(dense1)\n","# DENSE LAYER\n","dense2 = layers.Dense(256,activation=\"relu\")(drop2)\n","# DROPOUT LAYER\n","drop2 = layers.Dropout(0.3)(dense2)\n","# DENSE LAYER\n","dense3 = layers.Dense(128,activation=\"relu\")(drop2)\n","# DROPOUT LAYER\n","drop2 = layers.Dropout(0.4)(dense3)\n","# BATCHNORM LAYER\n","bn2  = layers.BatchNormalization()(drop2)\n","# DENSE LAYER\n","dense4 = layers.Dense(1,activation=\"linear\")(bn2)\n","# MODEL\n","model =  Model(inputs= [inp1,inp2,inp3,inp4,inp5,inp6],outputs=dense4)\n","\n","# SCHEDULE\n","def shedule(epoch,lr):\n","    if epoch<=2:\n","        return lr\n","    else:\n","        return lr*0.1\n","# CALLBACKS\n","lr = tf.keras.callbacks.LearningRateScheduler(shedule,verbose=1)\n","save = tf.keras.callbacks.ModelCheckpoint(\"content/drive/MyDrive/Blue Edtech/notebooks\",monitor=\"val_root_mean_squared_error\",mode=\"min\",save_best_only=True, save_weights_only=True,verbose=1)\n","earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\",min_delta= 0.01, patience=2,mode=\"min\" )\n","\n","model.compile(optimizer=\"adam\",loss=\"mse\",metrics=  [tf.keras.losses.MeanAbsoluteError(), tfa.metrics.r_square.RSquare(), tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.mean_absolute_percentage_error , tf.keras.metrics.mean_squared_logarithmic_error ])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3b7sbVso8ZW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665457504960,"user_tz":180,"elapsed":336577,"user":{"displayName":"lucio fontes","userId":"06607750709923064447"}},"outputId":"5824a05f-78be-4ac2-ad87-a50b8ce838d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n","Epoch 1/10\n","911/912 [============================>.] - ETA: 0s - loss: 0.7435 - mean_absolute_error: 0.5668 - r_square: -0.1929 - root_mean_squared_error: 0.8623 - mean_absolute_percentage_error: 20.5685 - mean_squared_logarithmic_error: 0.0869\n","Epoch 1: val_root_mean_squared_error improved from inf to 0.49799, saving model to content/drive/MyDrive/Blue Edtech/notebooks\n","912/912 [==============================] - 57s 60ms/step - loss: 0.7432 - mean_absolute_error: 0.5666 - r_square: -0.1924 - root_mean_squared_error: 0.8621 - mean_absolute_percentage_error: 20.5651 - mean_squared_logarithmic_error: 0.0869 - val_loss: 0.2480 - val_mean_absolute_error: 0.3835 - val_r_square: 0.6012 - val_root_mean_squared_error: 0.4980 - val_mean_absolute_percentage_error: 14.8761 - val_mean_squared_logarithmic_error: 0.0173 - lr: 0.0010\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n","Epoch 2/10\n","911/912 [============================>.] - ETA: 0s - loss: 0.2367 - mean_absolute_error: 0.3712 - r_square: 0.6203 - root_mean_squared_error: 0.4865 - mean_absolute_percentage_error: 13.9260 - mean_squared_logarithmic_error: 0.0162\n","Epoch 2: val_root_mean_squared_error improved from 0.49799 to 0.48674, saving model to content/drive/MyDrive/Blue Edtech/notebooks\n","912/912 [==============================] - 55s 61ms/step - loss: 0.2367 - mean_absolute_error: 0.3712 - r_square: 0.6203 - root_mean_squared_error: 0.4865 - mean_absolute_percentage_error: 13.9263 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.2369 - val_mean_absolute_error: 0.3712 - val_r_square: 0.6190 - val_root_mean_squared_error: 0.4867 - val_mean_absolute_percentage_error: 14.0224 - val_mean_squared_logarithmic_error: 0.0161 - lr: 0.0010\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n","Epoch 3/10\n","911/912 [============================>.] - ETA: 0s - loss: 0.2111 - mean_absolute_error: 0.3501 - r_square: 0.6613 - root_mean_squared_error: 0.4595 - mean_absolute_percentage_error: 13.1264 - mean_squared_logarithmic_error: 0.0145\n","Epoch 3: val_root_mean_squared_error improved from 0.48674 to 0.47783, saving model to content/drive/MyDrive/Blue Edtech/notebooks\n","912/912 [==============================] - 56s 61ms/step - loss: 0.2111 - mean_absolute_error: 0.3501 - r_square: 0.6613 - root_mean_squared_error: 0.4595 - mean_absolute_percentage_error: 13.1262 - mean_squared_logarithmic_error: 0.0145 - val_loss: 0.2283 - val_mean_absolute_error: 0.3652 - val_r_square: 0.6328 - val_root_mean_squared_error: 0.4778 - val_mean_absolute_percentage_error: 14.1125 - val_mean_squared_logarithmic_error: 0.0160 - lr: 0.0010\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n","Epoch 4/10\n","911/912 [============================>.] - ETA: 0s - loss: 0.1802 - mean_absolute_error: 0.3223 - r_square: 0.7110 - root_mean_squared_error: 0.4244 - mean_absolute_percentage_error: 12.0904 - mean_squared_logarithmic_error: 0.0125\n","Epoch 4: val_root_mean_squared_error improved from 0.47783 to 0.46731, saving model to content/drive/MyDrive/Blue Edtech/notebooks\n","912/912 [==============================] - 56s 62ms/step - loss: 0.1802 - mean_absolute_error: 0.3223 - r_square: 0.7110 - root_mean_squared_error: 0.4245 - mean_absolute_percentage_error: 12.0902 - mean_squared_logarithmic_error: 0.0125 - val_loss: 0.2184 - val_mean_absolute_error: 0.3543 - val_r_square: 0.6488 - val_root_mean_squared_error: 0.4673 - val_mean_absolute_percentage_error: 13.4310 - val_mean_squared_logarithmic_error: 0.0150 - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 1.0000000474974514e-05.\n","Epoch 5/10\n","911/912 [============================>.] - ETA: 0s - loss: 0.1727 - mean_absolute_error: 0.3153 - r_square: 0.7229 - root_mean_squared_error: 0.4156 - mean_absolute_percentage_error: 11.8285 - mean_squared_logarithmic_error: 0.0120\n","Epoch 5: val_root_mean_squared_error did not improve from 0.46731\n","912/912 [==============================] - 56s 61ms/step - loss: 0.1727 - mean_absolute_error: 0.3153 - r_square: 0.7229 - root_mean_squared_error: 0.4156 - mean_absolute_percentage_error: 11.8283 - mean_squared_logarithmic_error: 0.0120 - val_loss: 0.2195 - val_mean_absolute_error: 0.3555 - val_r_square: 0.6471 - val_root_mean_squared_error: 0.4685 - val_mean_absolute_percentage_error: 13.4684 - val_mean_squared_logarithmic_error: 0.0151 - lr: 1.0000e-05\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 1.0000000656873453e-06.\n","Epoch 6/10\n","911/912 [============================>.] - ETA: 0s - loss: 0.1722 - mean_absolute_error: 0.3148 - r_square: 0.7237 - root_mean_squared_error: 0.4150 - mean_absolute_percentage_error: 11.8155 - mean_squared_logarithmic_error: 0.0120\n","Epoch 6: val_root_mean_squared_error did not improve from 0.46731\n","912/912 [==============================] - 56s 61ms/step - loss: 0.1722 - mean_absolute_error: 0.3148 - r_square: 0.7237 - root_mean_squared_error: 0.4150 - mean_absolute_percentage_error: 11.8155 - mean_squared_logarithmic_error: 0.0120 - val_loss: 0.2196 - val_mean_absolute_error: 0.3558 - val_r_square: 0.6468 - val_root_mean_squared_error: 0.4687 - val_mean_absolute_percentage_error: 13.4978 - val_mean_squared_logarithmic_error: 0.0151 - lr: 1.0000e-06\n"]}],"source":["# FITTING THE MODEL\n","history = model.fit(x_train, y_train, validation_data= (x_test, y_test), epochs=10, batch_size = 1024, callbacks=[save,lr,earlystop])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vi1Ox_VBoBso"},"outputs":[],"source":["# CRIANDO A FUNÇÃO DE CÁLCULO DAS MÉTRICAS\n","\n","def print_avaliacao(obs, pred):\n","    print('R² = %.3f' % metrics.r2_score(obs, pred))\n","    print('MAPE = %.3f %%' % (100 * metrics.mean_absolute_percentage_error(obs, pred)))\n","    print('MAE = U$S %.2f' % (metrics.mean_absolute_error(obs, pred)))\n","    print('RMSE = U$S %.2f' % metrics.mean_squared_error(obs, pred)**0.5)\n","    print('RMSLE = %.4f' % metrics.mean_squared_log_error(obs, pred,squared=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uUVWwdjnYnD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665457569687,"user_tz":180,"elapsed":64734,"user":{"displayName":"lucio fontes","userId":"06607750709923064447"}},"outputId":"6cb4b40a-6dab-487c-b026-16deeae8aa9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["R² = 0.489\n","MAPE = 38.537 %\n","MAE = U$S 10.03\n","RMSE = U$S 27.66\n","RMSLE = 0.4418\n"]}],"source":["# Predição e avaliação da amostra teste\n","\n","y_pred = np.exp(model.predict(x_test))\n","print_avaliacao(base_test.price,y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7Wn8et7oeCe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665457593216,"user_tz":180,"elapsed":23536,"user":{"displayName":"lucio fontes","userId":"06607750709923064447"}},"outputId":"e913c6ab-b65e-423e-a84d-8a0bdbed8fcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["R² = 0.465\n","MAPE = 38.540 %\n","MAE = U$S 9.95\n","RMSE = U$S 28.03\n","RMSLE = 0.4421\n"]}],"source":["# Predição e avaliação da amostra de validação\n","\n","y_pred_val = np.exp(model.predict(x_val))\n","print_avaliacao(base_val.price,y_pred_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l62o0yw7HOie"},"outputs":[],"source":["# Venda seu produto!\n","\n","# produto = int(input('Digite índice do produto cadastrado: \\n'))\n","# select = (base_test.index == produto)\n","# produto_cadastrado = base_test[select]\n","# produto_cadastrado[['name', 'category_1', 'category_2', 'category_3', 'item_description', 'item_condition_id', 'brand_name', 'price']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7AOLks5Me-Y"},"outputs":[],"source":["# item_selecionado = [x_test[0][select],x_test[1][select],x_test[2][select],x_test[3][select],x_test[4][select],x_test[5][select],x_test[6][select],x_test[7][select]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2UNQOPrLFC1"},"outputs":[],"source":["# Valor sugerido\n","\n","# preco_sugerido = float(np.exp(model.predict(item_selecionado)))\n","# print(f' Sugerimos um valor entre U${np.round(preco_sugerido*0.8,2)} e U${np.round(preco_sugerido*1.2,2)}, um preço ideal seria U${np.round(preco_sugerido,2)}')"]},{"cell_type":"markdown","metadata":{"id":"cGyiIzArLz4U"},"source":["### Referência\n","\n","https://github.com/pushapgandhi/Mercari_Price_Prediction/blob/main/Deep_Learning_Model.ipynb\n","\n","https://towardsdatascience.com/mercari-price-prediction-challenge-3a8ea00a7d33\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}